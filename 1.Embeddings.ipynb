{"cells":[{"cell_type":"markdown","source":["# Prepare data and embeddings\n","\n","We used this notebook to prepare our dataset. We clip it to the range 1950-1999, create and save all relevant year, scene and test/train labels and make image embeddings using OpenCLIP."],"metadata":{"id":"4ANHd11wP1yw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqskX8XdxVFP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689760529938,"user_tz":-120,"elapsed":20738,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"82a38064-4811-4870-f51f-edf0f1787044"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open_clip_torch\n","  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.15.2+cu118)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2022.10.31)\n","Collecting ftfy (from open_clip_torch)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.65.0)\n","Collecting huggingface-hub (from open_clip_torch)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece (from open_clip_torch)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (3.20.3)\n","Collecting timm (from open_clip_torch)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open_clip_torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open_clip_torch) (16.0.6)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (23.1)\n","Collecting safetensors (from timm->open_clip_torch)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n","Installing collected packages: sentencepiece, safetensors, ftfy, huggingface-hub, timm, open_clip_torch\n","Successfully installed ftfy-6.1.1 huggingface-hub-0.16.4 open_clip_torch-2.20.0 safetensors-0.3.1 sentencepiece-0.1.99 timm-0.9.2\n"]}],"source":["!pip install open_clip_torch\n","import open_clip\n","\n","import torch\n","from PIL import Image\n","import random\n","import json\n","import pickle\n","import os\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hU0AMwgBxVFX"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["# helpers to load and save pickle and json files\n","\n","def loadPKL(path):\n","  infile = open(path,'rb')\n","  X = pickle.load(infile)\n","  print('Loaded ' + path.split('/')[-1])\n","  return X\n","\n","def savePKL(data, path):\n","  with open(path, 'wb') as f:\n","    pickle.dump(data, f)\n","  print('Saved ' + path.split('/')[-1])\n","\n","def loadJSON(path):\n","  infile = open(path,'rb')\n","  X = json.load(infile)\n","  print('Loaded ' + path.split('/')[-1])\n","  return X\n","\n","def saveJSON(data, path):\n","  with open(path, 'w') as jsonfile:\n","    json.dump(data, jsonfile)\n","  print('Saved ' + path.split('/')[-1])"],"metadata":{"id":"Sg0nXN-bAMzQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GyiLaJw0xVFb"},"source":["# Make labels"]},{"cell_type":"markdown","metadata":{"id":"vTHkDPyOxVFZ"},"source":["## Load metadata"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ov0JlvbD3qaI","executionInfo":{"status":"ok","timestamp":1689760601055,"user_tz":-120,"elapsed":23018,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"4058fc60-62cc-4f2d-a480-1c9b30effd38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_folder = '/content/drive/MyDrive/dating-images/data/'"],"metadata":{"id":"t6g0V2nA369L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDQypS1PxVFa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689763209673,"user_tz":-120,"elapsed":889,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"7d97c254-8a9e-4ca7-a518-f48f92698a72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded all_paths_dates.json\n"]}],"source":["image_data = loadJSON(data_folder + 'all_paths_dates.json')\n","\n","random.seed(20) # shuffle the data with a set seed\n","random.shuffle(image_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYrRyAwaxVFb"},"outputs":[],"source":["years = []\n","scenes = []\n","\n","for path, date in image_data:\n","  year = int(date.split('-')[-1])\n","  scene = path.split('/')[-2]\n","  years.append(year)\n","  scenes.append(scene)"]},{"cell_type":"markdown","source":["## Append labels to metadata"],"metadata":{"id":"Vxcewanr1U7j"}},{"cell_type":"code","source":["for i, item in enumerate(image_data):\n","  item.append(years[i])\n","  item.append(scenes[i])"],"metadata":{"id":"Bcf2j8ua1SKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clip image data within the range 1950-1999\n","\n","As the years prior to 1950 and after 2000 contained few images, we removed all data outside of this range."],"metadata":{"id":"P4bAlY7txRM1"}},{"cell_type":"code","source":["print(\"Total images: \" + str(len(image_data)))\n","\n","for i, year in reversed(list(enumerate(years))):\n","  if year < 1950 or year >= 2000:\n","    image_data.pop(i)\n","\n","print(\"Images in range 1950-1999: \" + str(len(image_data)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mjefskkixc4X","executionInfo":{"status":"ok","timestamp":1689763224974,"user_tz":-120,"elapsed":405,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"0680996e-2574-4a8c-a970-e9ae25016178"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total images: 43039\n","Images in range 1950-1999: 39866\n"]}]},{"cell_type":"markdown","source":["## Make test/train split\n","\n","We made a stratified train/test split based on the clipped year labels."],"metadata":{"id":"3oMvHNMT6eAE"}},{"cell_type":"code","source":["indices = list(np.arange(len(image_data)))\n","years = list(zip(*image_data))[2]"],"metadata":{"id":"Q2d9c1hG7E5L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, _, _ = train_test_split(indices, years, test_size=0.2, stratify=years, random_state=123)"],"metadata":{"id":"PECpV2hA6xs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(X_train))\n","print(len(X_test))\n","\n","print(X_train[:10])\n","print(X_test[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qn8vPGSu8KKn","executionInfo":{"status":"ok","timestamp":1689763309148,"user_tz":-120,"elapsed":433,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"05ce1de5-85e0-4f54-eb80-e1f4716f98cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["31892\n","7974\n","[13455, 22023, 29007, 38267, 32589, 23269, 34341, 33669, 39502, 11583]\n","[13198, 21682, 24509, 27532, 13036, 3068, 22977, 15677, 4527, 23582]\n"]}]},{"cell_type":"markdown","source":["Assign a `'train'` or `'test'` label to each of the images. We used these to make the train and test sets for all of our experiments."],"metadata":{"id":"uEyT8jC8PpNF"}},{"cell_type":"code","source":["for i, item in enumerate(image_data):\n","  if (i in X_train) and (i in X_test):\n","    print(\"Index \" + str(i) + \" is in both train and test sets! Check train/test split.\") # this shouldn't happen\n","    break\n","  if i in X_train:\n","    item.append('train')\n","    continue\n","  if i in X_test:\n","    item.append('test')\n","    continue"],"metadata":{"id":"E3VI2y4u8XpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(image_data))\n","print(image_data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tjm-JDcrxHaV","executionInfo":{"status":"ok","timestamp":1689763812935,"user_tz":-120,"elapsed":513,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"7a7fa860-d399-42d3-927d-5e4871e07bf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["39866\n","['../scene_detection/images/soccer/NL-HlmNHA_1478_20083K00_22.jpg', '19-11-1980', 1980, 'soccer', 'train']\n"]}]},{"cell_type":"markdown","source":["Save a copy of the image metadata including the year, scene and train/test labels."],"metadata":{"id":"lHtJ3Km4QIWp"}},{"cell_type":"code","source":["saveJSON(image_data, data_folder + 'image_data.json')"],"metadata":{"id":"vVmcE4q8_gIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make embeddings\n","\n","The dataset can be placed in `./data/scene_detection/images/`, where all images are in the corresponding `scene` folder.\n","\n","In our experiments we repeated this to create embeddings with the colorized images."],"metadata":{"id":"eUdCPJJunx3Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxMvbkHhxVFX"},"outputs":[],"source":["def get_image_embedding(image_path):\n","    image = Image.open(image_path)\n","    tensor = preprocess(image).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        embedding = model.encode_image(tensor)\n","    return embedding.flatten().cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"LxqUg-HxxVFX"},"source":["### Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etZmRDe9xVFY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687336028329,"user_tz":-120,"elapsed":14857,"user":{"displayName":"Alexandra Barancova","userId":"12338395437089192895"}},"outputId":"107b10a9-26f5-4c94-bbf4-9015f293747a"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 605M/605M [00:06<00:00, 94.1MiB/s]\n"]}],"source":["model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n","tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')"]},{"cell_type":"code","source":["X = []\n","\n","for image in image_data[:5]:\n","  path = data_folder + image[0].split('../')[-1]\n","  embedding = get_image_embedding(path)\n","  X.append(embedding)"],"metadata":{"id":"vlNfnrz2n1cJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j3OPqs3vxVFb"},"source":["Save a copy of the embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ub_FqgQnxVFc"},"outputs":[],"source":["savePKL(X, data_folder + 'embeddings.pkl')"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}